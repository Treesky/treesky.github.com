---
layout: post
title: "margin_auc"
date: 2014-02-19 13:26
comments: true
categories: 
---

最近在想一些关于评价标准的问题，就是什么是一个 “好” 的分类器。机器学习中关于好的定义和标准有很多，比如最早的 accuracy，之后的 precision，recall，F1等等。不同的应用方法体现了我们对 **好** 这个概念的不同认识。

	事实上在一个真实的机器学习问题中，如何确定一个 “好” 的概念，是最困难和重要的部分。

随着机器学习应用的发展，现实中很多应用都有着类别不平衡等各种问题，比如在广告点击率预估的问题中，clk 和 noclk 之间的数据差别是很大的，即使是凤巢首条广告 10%+ 的点击率，也依然有着较为严重的类别不平衡的问题； 这就导致了 precision 等评价指标并不能代表现实应用意义上的好。于是机器学习界引入了 AUC 作为评价指标。

AUC 的全称应该是 Area Under ROC Cruve. 就是 ROC 曲线的线下面积。ROC曲线则是以(TPR，FPR) 的点作为连线，画出的一条曲线。

$$
TPR = \cfrac{TP}{TP+FN} \\
FPR = \cfrac{FP}{FP+TN}
$$

在分类问题中这种计算方法，其实就是我们把决策面在测试集中不停的移动，然后看在不同的决策面下，判定为 + 的那些 instance 中，有多少是真的正例(TP)，有多少是假的正例（FP）。比如在广告预估系统中，我们预估出一个 instance 被点击的概率 p，然后我们不停的改变判定为点击的 p 的阈值，比如第一次计算 p_threshold = 0.9 的情况下，我们计算所有 p > 0.9 的 instance 中，有多少 instance 是真的点击，而有多少的 instance是未点击。然后经过 (TP+FN, FP+TN) 的归一化之后就是一个点。然后我们不停的改变 p_threshold 的阈值得到一系列点，最后把这些点连成 ROC 曲线，计算 ROC 曲线线下面积，得到AUC。

这种计算方法隐含的一个假设就是 **排在最前面的那些 instance 的准确性是最重要的**，因为在累加计算的过程中，第一个点总会累加到后面的所有点上，因此头部的正确性是非常重要的。而在实际应用的过程中（比如广告和视频推荐），我们会发现其实排在最前面的那些 instance 之间的相对关系来说，并不是最重要的。比如广告A 和 广告B 的预估 CTR 分别为 0.9 和 0.8，其实我们根本不关注 0.9 和 0.8 这两个 instance 谁更重要，因为这两个广告如果高的预估CTR， 不管谁比谁更高，最终都是会展现出去的。真正值得我们关心的是在分界面附近的那些广告， 即该广告的展现和不展现都是在模棱两可的时候，如果我们的分类器能把这些 case 给区分的比较清楚，这才是真正的善莫大焉。

因此我们计算一个 dist-auc, 也就是对原始的 auc 做一些计算上的改变，我们从分界面附近的分类器算起，从分界面往两边计算，计算出一个整体的 auc 出来，这样就可以使得模型的计算方法和我们的线上效果之间的关系更加密切。

回过头来看，这种不关心远离分界面之间的 instance 的计算方法，其实有点类似 margin 的思路，那么是不是如果我们使用 hinge-loss，来代替常规的 log-loss，在广告预估系统上会取得一个更好的效果呢？