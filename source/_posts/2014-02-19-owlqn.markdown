---
layout: post
title: "owlqn"
date: 2014-02-19 01:09
comments: true
categories: 
---

在我们优化一个 loss function 的时候最自然的想法就是 gradient descent，毕竟 gradient 指向的是在当前点上的最速下降方向。

但是 gradient descent 的 local convergence 的速度非常慢，这部分是因为 gradient descent 是一种 “贪婪” 的做法，每次的时候我们都只考虑当前点的最速下降方向。

除了 梯度 方向之外，还有很多方向可以用来作为搜索方向。这里面最有名的可能就是 Newton direction。

Newton direction 考虑了如下问题

$$
f(x_k + p) \approx f_k + p^T \nabla f_k + \cfrac{1}{2} p^T \nabla ^2 f_k p = m_k(p)
$$

当前的点是 $x_k$ 我们需要考虑的问题就是如果让 $x_k$ 在方向 $p$ 上移动单位长度，使得我们的函数值变的更小。
我们对 $f(x_k + p) $ 做二阶泰勒展式展开，就可以得到等号右边的形式。然后对这个泰勒展示导数 = 0，可以得到

$$
p = -\nabla ^2 f_k^{-1} \nabla f_k
$$

牛顿法在 local convergence 阶段有很好的收敛速度，但是 $ \nabla ^2 f_k $ 也就是我们所说的 hessian 矩阵，求解起来特别麻烦，于是我们就想通过一些近似的方法来求解 $ \nabla ^2 f_k $

一个简单的思路就是，二阶导数是一阶导数的导数，也就是说

$$
\nabla ^2 f_k^{-1} = B_{k+1} (x_{k+1} - x_k) = \nabla f_{k+1} - \nabla f_k 
$$

令 

$$
s_k = x_ {k+1} - x_k \\
y_k = \nabla f_{k+1} - \nabla f_k 
$$ 

我们就得到 $B_{k+1} s_k = y_k$ 这就是著名的切线公式。

注意我们在得到 newton direction 的过程中，需要的一个条件就是hessian 矩阵正定；同是考虑到，求偏导次序不影响结果，因此hessian 矩阵还有正对称的性质。

满足 $B_{k+1} s_k = y_k$ 的矩阵有无数，这个方程有 $n^2$ 的自由度，考虑到对称的性质，这个方程还有 $n(n-1)/2$ 的自由度；正定的性质可以额外引入 $n$ 个约束，即所有的 principal minors 应该是正的。但是这依然不足以唯一确定满足切线方程的矩阵。因此我们**强行** 引入了一个需求，就是 $B$ 和 $B_k$ 尽量“相似”

$$
\min \limits _B \| B - B_k \| \\
\text{subject to   } B = B^T, B s_k = y_k
$$

根据 “相似性” 也就是 norm 的定义不同，我们可以得到不同的 伪牛顿法，这里的相似性定义是`Frobenius norm`:

$$
\| A \| _w = \| W^{1/2}AW{1/2} ||_F
$$

$$
 \| C \| ^2_F = \sum _{i=1}^n \sum _{j=1}^n c_{ij}^2
$$

$W$ 的选择只要满足 $W y_k = S_k$ 即可。

这里我们取 $W = G_k^{-1}$

$$
G_k = [ \int_0^1 \nabla ^2 f(x_k + m \alpha _k p_k) dm]
$$

确定了 norm 的定义之后，我们求解 $B_k$ 就可以得到 DFP 伪牛顿法如下：

$$
B_{k+1} = (I - r_k y_k s_k ^T) B_k (I - r_k s_k y_k ^T) + r_k y_k y_k^T, \\
r_k = \cfrac{1}{y_k^T s_k}.
$$

考虑到我们最终用的时候，用的是 $B_k^{-1}$ 而不是 $B_k$ 本身，因此我们可以考虑直接近似 $B_k^{-1}$ 而不是近似 $B_k$ 然后再求逆矩阵。即我们直接求解 $H_{k+1} y_k = s_k$

$$
\min \limits_H \| H - H_k \| \\
\text{subject to } H = H^T, H y_k = s_k.
$$ 

一样的 norm 求解得到

$$
H_{k+1} = (I - \rho _k s_k y_k^T)H_k(I- \rho _k y_k s_k^T) + \rho _k s_k s_k^T, \\
\rho _k = \cfrac{1}{y_k^T s_k}
$$

这就是著名的BFGS算法。可能BFGS算法是所有的伪牛顿法中，普遍来说效果比较好的。

BFGS算法虽然很好，但是要存储 $H_{k+1}$ 这个矩阵的大小是 $n(n+1)/2$ ，这个空间在 $n$ 很大的时候，是难以接受的。

通过分析 BFGS 的更新公式，我们可以看到每次更新都只依赖于 $H_k, s_k, y_k$，如果我们把之前的 $H_0, 以及每一步的s_k, y_k$ 都保存下来的话，每次使用的时候，我们就可以现算出一个 $r = H_k \nabla f_k$ 的向量出来。

通过精心的算法设计，就得到了LBFGS two-loop recursion 算法
`http://www.cnblogs.com/downtjs/archive/2013/07/29/3222643.html`

再接下来就是通过使用 sub gradient 代替 gradient，解决了 l1-norm 不可导的问题之后，就得到了 OWL-QN 算法。 